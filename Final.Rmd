---
title: "Final document"
author: "Rebecca Holm"
date: "`r Sys.Date()`"
output: html_document
---
```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
```


# Non Cross-Fitting Single-learner 

```{r}
start_time_ncfsl <- Sys.time()  

set.seed(8)
n <- 10000
simulations_ncfsl <- 300

true_ATE_ncfsl <- numeric(simulations_ncfsl)
ATE_naive_ncfsl <- numeric(simulations_ncfsl)
EIF_true_ncfsl <- numeric(simulations_ncfsl)
EIF_ncfsl <- numeric(simulations_ncfsl)
ATE_onestep_ncfsl <- numeric(simulations_ncfsl)

OS_lb_dsl_cf <- numeric(simulations_ncfsl)
OS_ub_dsl_cf <- numeric(simulations_ncfsl)
naive_lb_dsl_cf <- numeric(simulations_ncfsl)
naive_ub_dsl_cf <- numeric(simulations_ncfsl)
true_lb_dsl_cf <- numeric(simulations_ncfsl)
true_ub_dsl_cf <- numeric(simulations_ncfsl)

logit <- function(x) {
  exp(x) / (1 + exp(x))
}

for (i in 1:simulations_ncfsl) {
    set.seed(8+i)
    w1 <- rpois(n, 0.1)       # PrevAcuteCS - represents a variable of 3 levels
    w2 <- rbinom(n, 1, 0.16)  # PrevInstrumental
    w3 <- rpois(n, 0.07)      # PrevTiming - represents a varibales of 3 levels 
    w4 <- rbinom(n, 1, 0.42)  # PrevArgumented
    w5 <- rbinom(n, 1, 0.06)  # PrevPraeterm
    w6 <- rbinom(n, 1, 0.13)  # PrevEpisiotomy
    w7 <- rbinom(n, 1, 0.1)   # PrevInduced
    w8 <- rbinom(n, 1, 0.02)  # PrevRetained

    W <- cbind(w1, w2, w3, w4, w5, w6, w7, w8)
    
    prob_A <- logit(-2 + 0.5*W[,1]^2 + 0.8*W[,2] + 0.6*W[,3]^2 - 0.55*W[,4] - 0.1*W[,5] - 0.01*W[,6] - 0.9*W[,7] - 0.7*W[,8])
    A <- rbinom(n = nrow(W), size = 1, prob = prob_A)

    pY <- function(A, W) {
        logit(-4 + A - 0.9*W[,1] - 0.8*W[,2] - 0.7*W[,6])
    }
    
    prob_Y <- pY(A = A, W = W)
    Y <- rbinom(n = nrow(W), size = 1, prob = prob_Y)
    pY1 <- pY(A = 1, W = W)
    pY0 <- pY(A = 0, W = W)

    # true_ATE and true_EIF
    true_ATE_ncfsl[i] <- mean(pY1) - mean(pY0) 
    EIF_true_ncfsl[i] <- sd(pY1 - pY0- true_ATE_ncfsl[i] + A/prob_A * (Y - pY1) - (1-A)/(1- prob_A) *(Y-pY0))


    data_ncfsl <- data.frame(Y = Y, A = A, W)

    # learner 
    GLM1_ncfsl <- glm(Y ~ A + w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_ncfsl, family = "binomial")    
    
    data_1_ncfsl <- data_ncfsl
    data_0_ncfsl <- data_ncfsl
    data_1_ncfsl$A <- 1
    data_0_ncfsl$A <- 0
    
    # predictions for learner 
    predictions_GLM1_ncfsl <- predict(GLM1_ncfsl, newdata = data_1_ncfsl[, -1], type = "response")
    predictions_GLM0_ncfsl <- predict(GLM1_ncfsl, newdata = data_0_ncfsl[, -1], type = "response")
    
    # naive ATE 
    ATE_naive_ncfsl[i] <- mean(predictions_GLM1_ncfsl) - mean(predictions_GLM0_ncfsl)

    # learner for propensity
    GLM1_propensity_ncfsl <- glm(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_ncfsl[, -c(1,2)], family = "binomial")

    # predict propensity
    propensity_scores_ncfsl <- predict(GLM1_propensity_ncfsl, newdata = data_ncfsl[, -c(1,2)], type = "response")

    EIF_ncfsl[i] <- mean((data_ncfsl$A / propensity_scores_ncfsl) * (data_ncfsl$Y - predictions_GLM1_ncfsl) - ((1 - data_ncfsl$A) / (1 - propensity_scores_ncfsl)) * (data_ncfsl$Y - predictions_GLM0_ncfsl))
  
    # ATE onestep
    ATE_onestep_ncfsl[i] <- ATE_naive_ncfsl[i] + EIF_ncfsl[i]
    
    # stuff for confidence intervals 
        EIF_onestep_ncfsl <- (predictions_GLM1_ncfsl - predictions_GLM0_ncfsl - ATE_onestep_ncfsl[i] + data_ncfsl$A/propensity_scores_ncfsl * (data_ncfsl$Y - predictions_GLM1_ncfsl) - (1 - data_ncfsl$A)/(1- propensity_scores_ncfsl) *(data_ncfsl$Y-predictions_GLM0_ncfsl))
    
    EIF_naive_ncfsl <- (predictions_GLM1_ncfsl - predictions_GLM0_ncfsl - ATE_naive_ncfsl[i] + data_ncfsl$A/propensity_scores_ncfsl * (data_ncfsl$Y - predictions_GLM1_ncfsl) - (1 - data_ncfsl$A)/(1- propensity_scores_ncfsl) *(data_ncfsl$Y-predictions_GLM0_ncfsl))
    
  OS_lb_dsl_cf[i] <- ATE_onestep_ncfsl[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep_ncfsl)
  OS_ub_dsl_cf[i] <- ATE_onestep_ncfsl[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep_ncfsl)
  naive_lb_dsl_cf[i] <- ATE_naive_ncfsl[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_naive_ncfsl)
  naive_ub_dsl_cf[i] <- ATE_naive_ncfsl[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_naive_ncfsl)
  true_lb_dsl_cf[i] <- true_ATE_ncfsl[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_true_ncfsl)
  true_ub_dsl_cf[i] <- true_ATE_ncfsl[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_true_ncfsl)
}

mean(ATE_naive_ncfsl)
mean(ATE_onestep_ncfsl)
mean(true_ATE_ncfsl)

end_time_ncfsl <- Sys.time()
total_time_ncfsl <- end_time_ncfsl - start_time_ncfsl 
print(total_time_ncfsl) 
```

```{r}
# Print the results
print(paste("95% CI for Naive ATE: (", mean(naive_lb_dsl_cf), ", ", mean(naive_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for OneStep ATE: (", mean(OS_lb_dsl_cf), ", ", mean(OS_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for True ATE: (", mean(true_lb_dsl_cf), ", ", mean(true_ub_dsl_cf), ")", sep = ""))
```


```{r}
test_ncfsl <- sqrt(n) * (ATE_onestep_ncfsl - true_ATE_ncfsl)  

results_df_ncfsl <- data.frame(StandardizedDifference = test_ncfsl)

ggplot(results_df_ncfsl, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of OneStep ATE"), fill = "darkblue", alpha = 0.4, size = 0.4) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true_ncfsl)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +
  labs(
    title = "Comparison of OneStep ATE and True ATE, for Non Cross-Fitting, Single-Learner",
    x = "Standardized Difference (sqrt(n)*(OneStepATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +  # Adjust limits as necessary
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of OneStep ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Include histogram fill in the legend if necessary

```



```{r}
test_ncfsl <- sqrt(n) * (ATE_naive_ncfsl - true_ATE_ncfsl)  # standardized differences

results_df_ncfsl <- data.frame(StandardizedDifference = test_ncfsl)

ggplot(results_df_ncfsl, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of g-formula ATE"), fill = "darkblue", alpha = 0.4) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true_ncfsl)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +
  labs(
    title = "Comparison of g-formula ATE and True ATE, for Non Cross-Fitting, Single-Learner",
    x = "Standardized Difference (sqrt(n)*(g-formulaATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +  # Adjust limits as necessary
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of g-formula ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Include histogram fill in the legend if necessary
```


# Non Cross-Fitting with the Super-Learner Method


```{r}
start_time_ncfdsl <- Sys.time()  

set.seed(8)
n <- 10000
simulations <- 300
folds_cv <- 5 

fold_indices_cv <- sample(rep(1:folds_cv, length.out = n))

true_ATE_ncfdsl <- numeric(simulations)
ATE_naive_ncfdsl <- numeric(simulations)
EIF_ncfdsl <- numeric(simulations)
SubEIF_ncfdsl <- numeric(simulations)
EIF_true_ncfdsl <- numeric(simulations)
ATE_onestep_ncfdsl <- numeric(simulations)
OneStep_normal_ncfdsl <- numeric(simulations)
Naive_normal_ncfdsl <- numeric(simulations)
best_model_indices_ncfdsl <- numeric(simulations)
best_model_indices_A_ncfdsl <- numeric(simulations)
EIF_onestep_ncfdsl<- numeric(simulations)
EIF_onestep_ncfdsl<- numeric(simulations)

predictions_A_1_ncfdsl <- numeric(n)
predictions_A_2_ncfdsl <- numeric(n)
predictions_A_3_ncfdsl <- numeric(n)
predictions_A_4_ncfdsl <- numeric(n)
predictions_Y_1_ncfdsl <- numeric(n)
predictions_Y_2_ncfdsl <- numeric(n)
predictions_Y_3_ncfdsl <- numeric(n)
predictions_Y_4_ncfdsl <- numeric(n)

OS_lb_dsl_cf <- numeric(simulations)
OS_ub_dsl_cf <- numeric(simulations)
naive_lb_dsl_cf <- numeric(simulations)
naive_ub_dsl_cf <- numeric(simulations)
true_lb_dsl_cf <- numeric(simulations)
true_ub_dsl_cf <- numeric(simulations)

models_A_ncfdsl <- list()
models_ncfdsl <- list()

logit <- function(x) {
  exp(x)/(1+exp(x))
}

for (i in 1:simulations) {
    set.seed(8+i)
    w1 <- rpois(n, 0.1)      # PrevAcuteCS - represents a variable of 3 levels
    w2 <- rbinom(n, 1, 0.16)  # PrevInstrumental
    w3 <- rpois(n, 0.07)      # PrevTiming - represents a varibales of 3 levels 
    w4 <- rbinom(n, 1, 0.42)  # PrevArgumented
    w5 <- rbinom(n, 1, 0.06)  # PrevPraeterm
    w6 <- rbinom(n, 1, 0.13)  # PrevEpisiotomy
    w7 <- rbinom(n, 1, 0.1)   # PrevInduced
    w8 <- rbinom(n, 1, 0.02)  # PrevRetained

    W <- cbind(w1, w2, w3, w4, w5, w6, w7, w8)
    
    prob_A_ncfdsl <- logit(-2 + 0.5*W[,1]^2 + 0.8*W[,2] + 0.6*W[,3]^2 - 0.55*W[,4] - 0.1*W[,5] - 0.01*W[,6] - 0.9*W[,7] - 0.7*W[,8])
    A <- rbinom(n = nrow(W), size = 1, prob = prob_A_ncfdsl)

    pY_ncfdsl <- function(A, W) {
        logit(-4 + A - 0.9*W[,1] - 0.8*W[,2] - 0.7*W[,6])
    }
    
    prob_Y_ncfdsl <- pY_ncfdsl(A = A, W = W)
    Y <- rbinom(n = nrow(W), size = 1, prob = prob_Y_ncfdsl)
    pY1_ncfdsl <- pY_ncfdsl(A = 1, W = W)
    pY0_ncfdsl <- pY_ncfdsl(A = 0, W = W)
    
    # true_ATE and true_EIF
    true_ATE_ncfdsl[i] <- (mean(pY1_ncfdsl) - mean(pY0_ncfdsl)) 
    EIF_true_ncfdsl[i] <- sd(pY1_ncfdsl - pY0_ncfdsl - true_ATE_ncfdsl[i] + A/prob_A_ncfdsl * (Y - pY1_ncfdsl) - (1-A)/(1- prob_A_ncfdsl) *(Y-pY0_ncfdsl))


    data <- data.frame(Y = Y, A = A, W)

    
for (fold_cv in 1:folds_cv) {
    train_indices_cv <- which(fold_indices_cv != fold_cv)
    test_indices_cv <- which(fold_indices_cv == fold_cv)
    
    data_train_cv <- data[train_indices_cv, ]
    data_test_cv <- data[test_indices_cv, ]

    models_ncfdsl <- list(
      glm1_Y_ncfdsl <- glm(Y ~ A + w1 + w2 + w3 * w4 + w5 + w7 + w8, data = data_train_cv, family = "binomial"),
      glm2_Y_ncfdsl <- glm(Y ~ A + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
      glm3_Y_ncfdsl <- glm(Y ~ A + w1 + w2 + w3 + w4 + w6 + w7 * w8, data = data_train_cv, family = "binomial"),
      glm4_Y_ncfdsl <- glm(Y ~ A + w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial")      )
    offset <- (fold_cv - 1) * length(test_indices_cv)
    predictions_Y_1_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm1_Y_ncfdsl, newdata = data_test_cv, type = "response")
    predictions_Y_2_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm2_Y_ncfdsl, newdata = data_test_cv, type = "response")
    predictions_Y_3_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm3_Y_ncfdsl, newdata = data_test_cv, type = "response")
    predictions_Y_4_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm4_Y_ncfdsl, newdata = data_test_cv, type = "response")
    }

    MSEs_ncfdsl <- cbind(mean((data_test_cv$Y - predictions_Y_1_ncfdsl)^2), mean((data_test_cv$Y - predictions_Y_2_ncfdsl)^2),
                    mean((data_test_cv$Y - predictions_Y_3_ncfdsl)^2), mean((data_test_cv$Y - predictions_Y_4_ncfdsl)^2))
    
    best_model_index_ncfdsl <- which.min(MSEs_ncfdsl)
    best_model_ncfdsl <- models_ncfdsl[[best_model_index_ncfdsl]]

    best_model_indices_ncfdsl[i] <- best_model_index_ncfdsl  
    table(best_model_indices_ncfdsl)
    
    data_1 <- data
    data_0 <- data
    data_1$A <- 1
    data_0$A <- 0

         # We then use the best model to predict Y=1 and Y=0
        predictions_GLM1_ncfdsl <- predict(best_model_ncfdsl, newdata = data_1[, -1], type = "response")
        predictions_GLM0_ncfdsl <- predict(best_model_ncfdsl, newdata = data_0[, -1], type = "response")
    
    
    # Save my ATE_naive in a vector
    ATE_naive_ncfdsl[i] <- mean(predictions_GLM1_ncfdsl) - mean(predictions_GLM0_ncfdsl)

    
for (fold_cv in 1:folds_cv) {
    train_indices_cv <- which(fold_indices_cv != fold_cv)
    test_indices_cv <- which(fold_indices_cv == fold_cv)
    
    data_train_cv <- data[train_indices_cv, ]
    data_test_cv <- data[test_indices_cv, ]

    models_A_ncfdsl <- list(
    glm1_A_ncfdsl <- glm(A ~ w1 + w2 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
    glm2_A_ncfdsl <- glm(A ~ w1 * w2 + w3 + w4 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
    glm3_A_ncfdsl <- glm(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 * w8, data = data_train_cv, family = "binomial"),
    glm4_A_ncfdsl <- glm(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial")
    )
    offset <- (fold_cv - 1) * length(test_indices_cv)
    predictions_A_1_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm1_A_ncfdsl, newdata = data_test_cv, type = "response")
    predictions_A_2_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm2_A_ncfdsl, newdata = data_test_cv, type = "response")
    predictions_A_3_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm3_A_ncfdsl, newdata = data_test_cv, type = "response")
    predictions_A_4_ncfdsl[offset + seq_along(test_indices_cv)] <- predict(glm4_A_ncfdsl, newdata = data_test_cv, type = "response")
}

        MSEs_A_ncfdsl <- cbind(mean((data_test_cv$A - predictions_A_1_ncfdsl)^2), mean((data_test_cv$A - predictions_A_2_ncfdsl)^2), mean((data_test_cv$A - predictions_A_3_ncfdsl)^2), mean((data_test_cv$A - predictions_A_4_ncfdsl)^2))
   
        best_model_index_A_ncfdsl <- which.min(MSEs_A_ncfdsl)
        best_model_A_ncfdsl <- models_A_ncfdsl[[best_model_index_A_ncfdsl]]
        
        best_model_indices_A_ncfdsl[i] <- best_model_index_A_ncfdsl 
        table(best_model_indices_A_ncfdsl)        

    # Propensity score on best model

    propensity_scores_ncfdsl <- predict(best_model_A_ncfdsl, newdata = data[, -c(1,2)], type = "response")


    SubEIF_ncfdsl[i] <- mean((data$A / propensity_scores_ncfdsl) * (data$Y - predictions_GLM1_ncfdsl) - ((1 - data$A) / (1 - propensity_scores_ncfdsl)) * (data$Y - predictions_GLM0_ncfdsl))

  
    # Onestep ATE 
    ATE_onestep_ncfdsl[i] <- ATE_naive_ncfdsl[i] + SubEIF_ncfdsl[i]
    
    # CI stuff
    EIF_onestep_ncfdsl <- (predictions_GLM1_ncfdsl - predictions_GLM0_ncfdsl - ATE_onestep_ncfdsl[i] + data$A/propensity_scores_ncfdsl * (data$Y - predictions_GLM1_ncfdsl) - (1 - data$A)/(1- propensity_scores_ncfdsl) *(data$Y-predictions_GLM0_ncfdsl))
    
    EIF_naive_ncfdsl <- (predictions_GLM1_ncfdsl - predictions_GLM0_ncfdsl - ATE_naive_ncfdsl[i] + A/propensity_scores_ncfdsl * (Y - predictions_GLM1_ncfdsl) - (1 - A)/(1- propensity_scores_ncfdsl) *(Y-predictions_GLM0_ncfdsl))
    
  OS_lb_dsl_cf[i] <- ATE_onestep_ncfdsl[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep_ncfdsl)
  OS_ub_dsl_cf[i] <- ATE_onestep_ncfdsl[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep_ncfdsl)
  naive_lb_dsl_cf[i] <- ATE_naive_ncfdsl[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_naive_ncfdsl)
  naive_ub_dsl_cf[i] <- ATE_naive_ncfdsl[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_naive_ncfdsl)
  true_lb_dsl_cf[i] <- true_ATE_ncfdsl[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_true_ncfdsl)
  true_ub_dsl_cf[i] <- true_ATE_ncfdsl[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_true_ncfdsl)
}

mean(ATE_naive_ncfdsl)
mean(ATE_onestep_ncfdsl)
mean(true_ATE_ncfdsl)

end_time_ncfdsl <- Sys.time()  
total_time_ncfdsl <- end_time_ncfdsl - start_time_ncfdsl
print(total_time_ncfdsl)
```


```{r}
# Print the results
print(paste("95% CI for Naive ATE: (", mean(naive_lb_dsl_cf), ", ", mean(naive_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for OneStep ATE: (", mean(OS_lb_dsl_cf), ", ", mean(OS_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for True ATE: (", mean(true_lb_dsl_cf), ", ", mean(true_ub_dsl_cf), ")", sep = ""))
```


## Visualize the frequency of the best model selection for Y:
```{r}
model_freq_ncfdsl <- as.data.frame(table(best_model_indices_ncfdsl))
names(model_freq_ncfdsl) <- c("ModelIndex", "Frequency")

ggplot(data = model_freq_ncfdsl, aes(x = ModelIndex, y = Frequency)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(x = "Model Index", y = "Frequency", title = "Frequency of Best Model Selection for Y") +
    theme_minimal()
```
## Visualize the frequency of the best model selection for A:
```{r}
model_freq_ncfdsl_A <- as.data.frame(table(best_model_indices_A_ncfdsl))
names(model_freq_ncfdsl_A) <- c("ModelIndex", "Frequency")

ggplot(data = model_freq_ncfdsl_A, aes(x = ModelIndex, y = Frequency)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(x = "Model Index", y = "Frequency", title = "Frequency of Best Model Selection for A") +
    theme_minimal()
```



# try to implement the cross-validation step
```{r}
test_ncfdsl_os <- sqrt(n) * (ATE_onestep_ncfdsl - true_ATE_ncfdsl) 

results_df_ncfdsl_os <- data.frame(StandardizedDifference = test_ncfdsl_os)

ggplot(results_df_ncfdsl_os, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of OneStep ATE"), fill = "darkblue", alpha = 0.4) +  # Use aes for color
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true_ncfdsl)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +  # Use aes for color
  labs(
    title = "Comparison of OneStep ATE and True ATE for Non Cross-Fitting, Discrete Super-Learner",
    x = "Standardized Difference (sqrt(n)*(OneStepATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 12),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of OneStep ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Include histogram fill in the legend if necessary

```

```{r}
test_naive_ncsdsl <- sqrt(n) * (ATE_naive_ncfdsl - true_ATE_ncfdsl) 

results_df_naive_ncsdsl <- data.frame(StandardizedDifference = test_naive_ncsdsl)

ggplot(results_df_naive_ncsdsl, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of g-formula ATE"), fill = "darkblue", alpha = 0.4) +  # Use aes for color
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true_ncfdsl)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +  # Use aes for color
  labs(
    title = "Comparison of g-formula ATE and True ATE for Non Cross-Fitting, Discrete Super-Learner",
    x = "Standardized Difference (sqrt(n)*(g-formuaATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 12),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of g-formula ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Include histogram fill in the legend if necessary

```

# Cross-Fitting with Single-Learner method
```{r}
start_time_smcf <- Sys.time()  

n <- 10000
simulations <- 300
folds <- 5

true_ATE_smcf <- numeric(simulations)
ATE_naive_smcf <- numeric(simulations)
SubEIF_smcf <- numeric(simulations)
ATE_onestep_smcf <- numeric(simulations)
OneStep_normal_smcf <- numeric(simulations)
Naive_normal_smcf <- numeric(simulations)



ATE_naive_on_train_data_smcf <- numeric(folds)
ATE_naive_on_test_data_smcf <- numeric(folds)
SubEIF_folds_on_train_data_smcf <- numeric(folds)
SubEIF_folds_on_test_data_smcf <- numeric(folds)
EIF_true_smcf <- numeric(simulations)
EIF_onestep_smcf <- numeric(simulations)
EIF_naive_smcf <- numeric(simulations)



OS_lb_dsl_cf <- numeric(simulations)
OS_ub_dsl_cf <- numeric(simulations)
naive_lb_dsl_cf <-numeric(simulations)
naive_ub_dsl_cf <- numeric(simulations)
true_lb_dsl_cf <- numeric(simulations)
true_ub_dsl_cf <- numeric(simulations)


# Create my logit function
logit <- function(x) {
  exp(x) / (1 + exp(x))
}

# Create folds for cross-fitting
fold_indices <- sample(rep(1:folds, length.out = n))

for (i in 1:simulations) {
    set.seed(8 + i)
    w1 <- rpois(n, 0.1)      # PrevAcuteCS - represents a variable of 3 levels
    w2 <- rbinom(n, 1, 0.16)  # PrevInstrumental
    w3 <- rpois(n, 0.07)      # PrevTiming - represents a varibales of 3 levels 
    w4 <- rbinom(n, 1, 0.42)  # PrevArgumented
    w5 <- rbinom(n, 1, 0.06)  # PrevPraeterm
    w6 <- rbinom(n, 1, 0.13)  # PrevEpisiotomy
    w7 <- rbinom(n, 1, 0.1)   # PrevInduced
    w8 <- rbinom(n, 1, 0.02)  # PrevRetained

    W <- cbind(w1, w2, w3, w4, w5, w6, w7, w8)
    
    prob_A <- logit(-2 + 0.5*W[,1]^2 + 0.8*W[,2] + 0.6*W[,3]^2 - 0.55*W[,4] - 0.1*W[,5] - 0.01*W[,6] - 0.9*W[,7] - 0.7*W[,8])
    A <- rbinom(n = nrow(W), size = 1, prob = prob_A)

    pY <- function(A, W) {
        logit(-4 + A - 0.9*W[,1] - 0.8*W[,2] - 0.7*W[,6])
    }
    
    prob_Y <- pY(A = A, W = W)
    Y <- rbinom(n = nrow(W), size = 1, prob = prob_Y)
    pY1 <- pY(A = 1, W = W)
    pY0 <- pY(A = 0, W = W)
mean(pY1) - mean(pY0)
    # This is the true value I try to get. I save it in a vector 
    true_ATE_smcf[i] <- mean(pY1) - mean(pY0)
    EIF_true_smcf[i] <- sd(pY1 - pY0 - true_ATE_smcf[i] + A/prob_A * (Y - pY1) - (1 - A)/(1- prob_A) *(Y-pY0))
    
    # Create my data
    data <- data.frame(Y = Y, A = A, W)
    
    for (fold in 1:folds) {
        train_indices <- which(fold_indices != fold)
        test_indices <- which(fold_indices == fold)
        
        data_train <- data[train_indices, ]
        data_test <- data[test_indices, ]
        
        y_train <- data_train$Y
        y_test <- data_test$Y
        
        # Fit and prediction for Y
        GLM1_on_train_data_smcf <- glm(Y ~ A + w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train, family = "binomial")    
        data_1_test <- data_test
        data_0_test <- data_test
        data_1_test$A <- 1
        data_0_test$A <- 0

        predictions_1_on_test_data <- predict(GLM1_on_train_data_smcf, newdata = data_1_test[, -1], type = "response")
        predictions_0_on_test_data <- predict(GLM1_on_train_data_smcf, newdata = data_0_test[, -1], type = "response")
        
        ATE_naive_on_test_data_smcf[fold] <- mean(predictions_1_on_test_data) - mean(predictions_0_on_test_data)

        GLM1_propensity_train_smcf <- glm(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train[, -1], family = "binomial")

        propensity_scores_on_test_data_smcf <- predict(GLM1_propensity_train_smcf, newdata = data_test[, -c(1,2)], type = "response")

        
        SubEIF_folds_on_test_data_smcf[fold] <- mean((data_test$A / propensity_scores_on_test_data_smcf) * (y_test - predictions_1_on_test_data) - ((1 - data_test$A) / (1 - propensity_scores_on_test_data_smcf)) * (y_test - predictions_0_on_test_data))
    }

    # Save results
    ATE_naive_smcf[i] <- mean(ATE_naive_on_test_data_smcf)
    SubEIF_smcf[i] <- mean(SubEIF_folds_on_test_data_smcf)
    ATE_onestep_smcf[i] <- ATE_naive_smcf[i] + SubEIF_smcf[i]
    
    EIF_onestep_smcf <- (predictions_1_on_test_data - predictions_0_on_test_data - ATE_onestep_smcf[i] + A/propensity_scores_on_test_data_smcf * (Y - predictions_1_on_test_data) - (1 - A)/(1- propensity_scores_on_test_data_smcf) *(Y-predictions_0_on_test_data))
    
    EIF_naive_smcf <- (predictions_1_on_test_data - predictions_0_on_test_data - ATE_naive_smcf[i] + A/propensity_scores_on_test_data_smcf * (Y - predictions_1_on_test_data) - (1 - A)/(1- propensity_scores_on_test_data_smcf) *(Y-predictions_0_on_test_data))
    
  OS_lb_dsl_cf[i] <- ATE_onestep_smcf[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep_smcf)
  OS_ub_dsl_cf[i] <- ATE_onestep_smcf[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep_smcf)
  naive_lb_dsl_cf[i] <- ATE_naive_smcf[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_naive_smcf)
  naive_ub_dsl_cf[i] <- ATE_naive_smcf[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_naive_smcf)
  true_lb_dsl_cf[i] <- true_ATE_smcf[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_true_smcf)
  true_ub_dsl_cf[i] <- true_ATE_smcf[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_true_smcf)
}


mean(ATE_naive_smcf)
mean(ATE_onestep_smcf)
mean(true_ATE_smcf)

end_time_smcf <- Sys.time()  # Record end time
total_time <- end_time_smcf - start_time_smcf  # Compute total duration
print(total_time)  # Print the total computation time
```

```{r}
# Print the results
print(paste("95% CI for Naive ATE: (", mean(naive_lb_dsl_cf), ", ", mean(naive_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for OneStep ATE: (", mean(OS_lb_dsl_cf), ", ", mean(OS_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for True ATE: (", mean(true_lb_dsl_cf), ", ", mean(true_ub_dsl_cf), ")", sep = ""))
```


```{r}
test_smcf <- sqrt(n) * (ATE_onestep_smcf - true_ATE_smcf)  
results_df_smcf <- data.frame(StandardizedDifference = test_smcf)

ggplot(results_df_smcf, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of OneStep ATE"), fill = "darkblue", alpha = 0.4) +  # Use aes for color
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true_smcf)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +  # Use aes for color
  labs(
    title = "Comparison of OneStep ATE and True ATE for Cross-Fitting, Single-Leaner",
    x = "Standardized Difference (sqrt(n)*(OneStepATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +  # Adjust limits as necessary
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of OneStep ATE" = "darkblue", "Distribution of True EIF" = "darkorange2"))  # Define legend colors

```

```{r}
test_naive <- sqrt(n) * (ATE_naive_smcf - true_ATE_smcf)

results_df <- data.frame(StandardizedDifference = test_naive)

ggplot(results_df, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of g-formula ATE"), fill = "darkblue", alpha = 0.4) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true_smcf)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +
  labs(
    title = "Comparison of g-formula ATE and True ATE for Cross-Fitting, Single-Learner",
    x = "Standardized Difference (sqrt(n)*(g-formulaATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +  # Adjust limits as necessary
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of g-formula ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Include histogram fill in the legend if necessary

```

# Cross-fitting with the super-learner method with cross-validation step
```{r}
start_time <- Sys.time()  

set.seed(8)
n <- 10000
simulations <- 300
folds <- 5
folds_cv <- folds

true_ATE <- numeric(simulations)
ATE_naive <- numeric(simulations)
SubEIF <- numeric(simulations)
ATE_onestep <- numeric(simulations)
OneStep_normal <- numeric(simulations)
Naive_normal <- numeric(simulations)
EIF_true <- numeric(simulations)
best_model_indices <- numeric(simulations)
best_model_indices_A <- numeric(simulations)

ATE_naive_on_train_data <- numeric(folds)
ATE_naive_on_test_data <- numeric(folds)
SubEIF_folds_on_train_data <- numeric(folds)
SubEIF_folds_on_test_data <- numeric(folds)
ATE_onestep_on_test_data <- numeric(folds)

GLM_1_predictions_on_test_data <- numeric(simulations)
GLM_2_predictions_on_test_data <- numeric(simulations)
GLM_3_predictions_on_test_data <- numeric(simulations)
GLM_4_predictions_on_test_data <- numeric(simulations)

OS_lb_dsl_cf <- numeric(simulations)
OS_ub_dsl_cf <- numeric(simulations)
naive_lb_dsl_cf <-numeric(simulations)
naive_ub_dsl_cf <- numeric(simulations)
true_lb_dsl_cf <- numeric(simulations)
true_ub_dsl_cf <- numeric(simulations)

GLM_on_train_data <- list()
GLM_on_train_data_pro <- list()
MSEs <- matrix(NA, nrow = 4, ncol = folds_cv)  # Col = number of folds and rows are number of models 
models <- list()
models_A <- list()
MSEs_A <- matrix(NA, nrow = 4, ncol = folds_cv)  # Col = number of folds and rows are number of models 

logit <- function(x) {
  exp(x) / (1 + exp(x))
}

fold_indices <- sample(rep(1:folds, length.out = n))
fold_indices_cv <- sample(rep(1:folds_cv, length.out = n))

        predictions_A_1 <- numeric(n)
        predictions_A_2 <- numeric(n)
        predictions_A_3 <- numeric(n)
        predictions_A_4 <- numeric(n)
        predictions_Y_1 <- numeric(n)
        predictions_Y_2 <- numeric(n)
        predictions_Y_3 <- numeric(n)
        predictions_Y_4 <- numeric(n)

for (i in 1:simulations) {
    set.seed(8 + i)
    w1 <- rpois(n, 0.1)       # PrevAcuteCS - represents a variable of 3 levels
    w2 <- rbinom(n, 1, 0.16)  # PrevInstrumental
    w3 <- rpois(n, 0.07)      # PrevTiming - represents a varibales of 3 levels 
    w4 <- rbinom(n, 1, 0.42)  # PrevArgumented
    w5 <- rbinom(n, 1, 0.06)  # PrevPraeterm
    w6 <- rbinom(n, 1, 0.13)  # PrevEpisiotomy
    w7 <- rbinom(n, 1, 0.1)   # PrevInduced
    w8 <- rbinom(n, 1, 0.02)  # PrevRetained

    W <- cbind(w1, w2, w3, w4, w5, w6, w7, w8)
    
    prob_A <- logit(-2 + 0.5*W[,1]^2 + 0.8*W[,2] + 0.6*W[,3]^2 - 0.55*W[,4] - 0.1*W[,5] - 0.01*W[,6] - 0.9*W[,7] - 0.7*W[,8])
    A <- rbinom(n = nrow(W), size = 1, prob = prob_A)

    pY <- function(A, W) {
        logit(-4 + A - 0.9*W[,1]^2 - 0.8*W[,2] - 0.7*W[,6])
    }
    
    prob_Y <- pY(A = A, W = W)
    mean(prob_Y)
    Y <- rbinom(n = nrow(W), size = 1, prob = prob_Y)
    pY1 <- pY(A = 1, W = W)
    pY0 <- pY(A = 0, W = W)
    
    # True_ATE and True_EIF 
    true_ATE[i] <- mean(pY1) - mean(pY0)
    EIF_true[i] <- sd(pY1 - pY0 - true_ATE[i] + A/prob_A * (Y - pY1) - (1-A)/(1- prob_A) *(Y-pY0))

    data <- data.frame(Y = Y, A = A, W)
    
    for (fold in 1:folds) {
        train_indices <- which(fold_indices != fold)
        test_indices <- which(fold_indices == fold)
        
        data_train <- data[train_indices, ]
        data_test <- data[test_indices, ]
        
        y_train <- data_train$Y
        y_test <- data_test$Y
        
        
for (fold_cv in 1:folds_cv) {
    train_indices_cv <- which(fold_indices_cv != fold_cv)
    test_indices_cv <- which(fold_indices_cv == fold_cv)
    
    data_train_cv <- data[train_indices_cv, ]
    data_test_cv <- data[test_indices_cv, ]

    models <- list(
      glm1_Y <- glm(Y ~ A + w1 + w2 + w3 * w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
      glm2_Y <- glm(Y ~ A + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
      glm3_Y <- glm(Y ~ A + w1 + w2 + w3 + w4 + w6 + w7 * w8, data = data_train_cv, family = "binomial"),
      glm4_Y <- glm(Y ~ A + w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial")    
    )
    
    offset <- (fold_cv - 1) * length(test_indices_cv)
    predictions_Y_1[offset + seq_along(test_indices_cv)] <- predict(glm1_Y, newdata = data_test_cv, type = "response")
    predictions_Y_2[offset + seq_along(test_indices_cv)] <- predict(glm2_Y, newdata = data_test_cv, type = "response")
    predictions_Y_3[offset + seq_along(test_indices_cv)] <- predict(glm3_Y, newdata = data_test_cv, type = "response")
    predictions_Y_4[offset + seq_along(test_indices_cv)] <- predict(glm4_Y, newdata = data_test_cv, type = "response")
}

    MSEs <- cbind(mean((data_test_cv$Y - predictions_Y_1)^2), mean((data_test_cv$Y - predictions_Y_2)^2),
                    mean((data_test_cv$Y - predictions_Y_3)^2), mean((data_test_cv$Y - predictions_Y_4)^2))
    
    best_model_index <- which.min(MSEs)
    best_model <- models[[best_model_index]]

    best_model_indices[i] <- best_model_index  
    table(best_model_indices)
        
    data_1_test <- data_test
    data_0_test <- data_test
    data_1_test$A <- 1
    data_0_test$A <- 0

         # We then use the best model to predict Y=1 and Y=0
        predictions_1_on_test_data <- predict(best_model, newdata = data_1_test[, -1], type = "response")
        predictions_0_on_test_data <- predict(best_model, newdata = data_0_test[, -1], type = "response")

        # ATE_naive
        ATE_naive_on_test_data[fold] <- mean(predictions_1_on_test_data) - mean(predictions_0_on_test_data)
        

for (fold_cv in 1:folds_cv) {
    train_indices_cv <- which(fold_indices_cv != fold_cv)
    test_indices_cv <- which(fold_indices_cv == fold_cv)
    
    data_train_cv <- data[train_indices_cv, ]
    data_test_cv <- data[test_indices_cv, ]

    models_A <- list(
    glm1_A <- glm(A ~ w1 + w2 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
    glm2_A <- glm(A ~ w1 * w2 + w3 + w4 + w6 + w7 + w8, data = data_train_cv, family = "binomial"),
    glm3_A <- glm(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 * w8, data = data_train_cv, family = "binomial"),
    glm4_A <- glm(A ~ w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8, data = data_train_cv, family = "binomial")
    )
    offset <- (fold_cv - 1) * length(test_indices_cv)
    predictions_A_1[offset + seq_along(test_indices_cv)] <- predict(glm1_A, newdata = data_test_cv, type = "response")
    predictions_A_2[offset + seq_along(test_indices_cv)] <- predict(glm2_A, newdata = data_test_cv, type = "response")
    predictions_A_3[offset + seq_along(test_indices_cv)] <- predict(glm3_A, newdata = data_test_cv, type = "response")
    predictions_A_4[offset + seq_along(test_indices_cv)] <- predict(glm4_A, newdata = data_test_cv, type = "response")
}

        MSEs_A <- cbind(mean((data_test_cv$A - predictions_A_1)^2), mean((data_test_cv$A - predictions_A_2)^2), mean((data_test_cv$A - predictions_A_3)^2), mean((data_test_cv$A - predictions_A_4)^2))
   
        best_model_index_A <- which.min(MSEs_A)
        best_model_A <- models_A[[best_model_index_A]]
        
        best_model_indices_A[i] <- best_model_index_A 
        table(best_model_indices_A)
        
    
        # Predict propensity scores using the best model
        propensity_scores_on_test_data <- predict(best_model_A, newdata = data_test[,-c(1,2)], type = "response")

        SubEIF_folds_on_test_data[fold] <- mean((data_test$A / propensity_scores_on_test_data) * (y_test - predictions_1_on_test_data) - ((1 - data_test$A) / (1 - propensity_scores_on_test_data)) * (y_test - predictions_0_on_test_data))
        
        ATE_onestep_on_test_data[fold] <- ATE_naive_on_test_data[fold] + SubEIF_folds_on_test_data[fold]
}

    # Save results
    ATE_naive[i] <- mean(ATE_naive_on_test_data)
    SubEIF[i] <- mean(SubEIF_folds_on_test_data)
    ATE_onestep[i] <- mean(ATE_onestep_on_test_data)
    
    
    EIF_onestep <- (predictions_1_on_test_data - predictions_0_on_test_data - ATE_onestep[i] + A/propensity_scores_on_test_data * (Y - predictions_1_on_test_data) - (1 - A)/(1- propensity_scores_on_test_data) *(Y-predictions_0_on_test_data))
    
    EIF_naive <- (predictions_1_on_test_data - predictions_0_on_test_data - ATE_naive[i] + A/propensity_scores_on_test_data * (Y - predictions_1_on_test_data) - (1 - A)/(1- propensity_scores_on_test_data) *(Y-predictions_0_on_test_data))
    
  OS_lb_dsl_cf[i] <- ATE_onestep[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep)
  OS_ub_dsl_cf[i] <- ATE_onestep[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_onestep)
  naive_lb_dsl_cf[i] <- ATE_naive[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_naive)
  naive_ub_dsl_cf[i] <- ATE_naive[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_naive)
  true_lb_dsl_cf[i] <- true_ATE[i] - qnorm(0.975)*1/sqrt(n)*sd(EIF_true)
  true_ub_dsl_cf[i] <- true_ATE[i] + qnorm(0.975)*1/sqrt(n)*sd(EIF_true)


}

mean(ATE_naive)
mean(ATE_onestep)
mean(true_ATE)


end_time <- Sys.time()  # Record end time
total_time <- end_time - start_time  # Compute total duration
print(total_time)  # Print the total computation time
```


```{r}
# Print the results
print(paste("95% CI for Naive ATE: (", mean(naive_lb_dsl_cf), ", ", mean(naive_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for OneStep ATE: (", mean(OS_lb_dsl_cf), ", ", mean(OS_ub_dsl_cf), ")", sep = ""))
print(paste("95% CI for True ATE: (", mean(true_lb_dsl_cf), ", ", mean(true_ub_dsl_cf), ")", sep = ""))
```
## Visualize the frequency of the best model selection for Y:
```{r}
# Convert the table to a data frame and name the columns
model_freq <- as.data.frame(table(best_model_indices))
names(model_freq) <- c("ModelIndex", "Frequency")

# Use ggplot2 to plot the data
ggplot(data = model_freq, aes(x = ModelIndex, y = Frequency)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(x = "Model Index", y = "Frequency", title = "Frequency of Best Model Selection for Y") +
    theme_minimal()
```
## Visualize the frequency of the best model selection for A:
```{r}
# Convert the table to a data frame and name the columns
model_freq <- as.data.frame(table(best_model_indices_A))
names(model_freq) <- c("ModelIndex", "Frequency")

# Use ggplot2 to plot the data
ggplot(data = model_freq, aes(x = ModelIndex, y = Frequency)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(x = "Model Index", y = "Frequency", title = "Frequency of Best Model Selection for A") +
    theme_minimal()
```





```{r}
test <- sqrt(n) * (ATE_onestep - true_ATE)  # standardized differences

results_df <- data.frame(StandardizedDifference = test)

ggplot(results_df, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of OneStep ATE"), fill = "darkblue", alpha = 0.4) +  # Use aes for color
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +  # Use aes for color
  labs(
    title = "Comparison of OneStep ATE and True ATE for Cross-Fitting, Discrete Super-Learner",
    x = "Standardized Difference (sqrt(n)*(OneStepATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +  # Adjust limits as necessary
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.7),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of OneStep ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Define fill color for histogram in the legend


```


```{r}
test_naive <- sqrt(n) * (ATE_naive - true_ATE)  # standardized differences

results_df <- data.frame(StandardizedDifference = test_naive)

ggplot(results_df, aes(x = StandardizedDifference)) +
  geom_histogram(aes(y = ..density.., fill = "Histogram"), binwidth = 0.2, color = "black", fill = "steelblue", alpha = 0.3) +
  geom_density(aes(color = "Distribution of g-formula ATE"), fill = "darkblue", alpha = 0.4) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 0.5) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = mean(EIF_true)), 
                aes(color = "Distribution of True EIF"), 
                size = 1) +
  labs(
    title = "Comparison of g-formula ATE and True ATE for Cross-Fitting, Discrete Super-Learner",
    x = "Standardized Difference (sqrt(n)*(g-formulaATE - trueATE))",
    y = "Density",
    color = "Legend"  # Title for the color legend
  ) +
  xlim(c(-5, 5)) +  # Adjust limits as necessary
  ylim(c(0, 0.8)) +  # Adjust limits as necessary
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.8),
    legend.position = "bottom"  # Positioning the legend at the bottom
  ) +
  scale_color_manual(values = c("Distribution of g-formula ATE" = "darkblue", "Distribution of True EIF" = "darkorange2")) +
  scale_fill_manual(values = c("Histogram" = "steelblue"))  # Include histogram fill in the legend if necessary

```

